\relax 
\citation{Monaghan-1993}
\citation{Ogielski-Morgenstern-1985,Ogielski-1985,Cruz-2001,Condon-Ogielski-1985,Taiji-Ito-Suzuki-1988}
\citation{Jorg-Katzgraber-Krzakala-2008,Moore-2005,Young-Katzgraber-2004,Temesvari-2008,Katzgraber-2008,Sasaki-etal-2008,Sasaki-etal-2007,Larson-etal-2013,Banos2012,Katzgraber-2012,Katzgraber-Larson-Young-2009,Leuzzi-2009}
\@writefile{toc}{\contentsline {chapter}{Chapter\ 3\hskip 1em\relax GPU Implementation}{10}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Introduction}{10}}
\citation{CSTN-093}
\citation{2010CoPhC.181.1549B}
\citation{Preis:2009:GAM:1537305.1537344}
\citation{doi:10.1142/S0129183112400025,Weigel:2012:PPS:2151219.2151631}
\citation{2012arXiv1204.4134J}
\citation{Janus2-2013}
\citation{Binder-Young1986}
\citation{Edwards-Anderson1975}
\citation{Sherrington-Kirkpatrick1978}
\citation{Almedia-Thouless1978,Bray-Moore-1978}
\citation{Parisi-1979,Mezard-etal-1984,Parisi-1980a,Parisi-1980b,Parisi-1980c,Parisi-dirac-medal-2002}
\citation{Talagrand-2006,Guerra-2003}
\citation{Barahona-1982}
\citation{Swendsen-Wang-1986,Hukushima-Nemoto1996,Marinari-Parisi1992}
\citation{Ballesteros2000}
\citation{Harris-Lubensky-Chen-1976,Tasaki-1989,Green-Moore-Bray-1983}
\citation{Young-Katzgraber2004}
\citation{Young-Katzgraber-2004}
\citation{Banos2012}
\citation{Zorn-Herrmann-Rebbi-1981}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Theoretical Background}{12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Spin Glass}{12}}
\citation{Edwards-Anderson1975}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Edwards-Anderson Model}{13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}Single Spin Flip Metropolis Algorithm}{13}}
\citation{Houdayer-2001,Liang-1992,Jorg-2005}
\citation{Marinari-Parisi1992,Hukushima-Nemoto1996}
\citation{PhysRevLett.101.130603,1742-5468-2006-03-P03018,jcp/124/17/10.1063/1.2186639}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.4}Parallel Tempering}{14}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}The Framework}{14}}
\newlabel{section_framework}{{3.3}{14}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Schematic diagram of the free energy landscape. At high temperatures (small $\beta $) the barriers between configurations are reduced allowing the system to search through configurations more efficiently.}}{15}}
\newlabel{fig-pt}{{3.1}{15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Map Lattice Sites to GPU Threads}{16}}
\newlabel{section_framework_threads}{{3.3.1}{16}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces A demonstration of the 3D checkerboard decomposition. The blue and red sites are on different sub-lattices. Since the sites in a sub-lattice never directly interact with each other, it is permissible to update different sites in parallel. }}{16}}
\newlabel{fig_checkerboard}{{3.2}{16}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Map Temperatures Replicas to Bits}{16}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Many temperature replicas can be simulated simultaneously, each using an independent Monte Carlo process. These replicas may be exchanged after a configurable steps of updates. A single GPU thread block is responsible for updating all the Monte Carlo processes and manipulating the parallel tempering exchange. }}{17}}
\newlabel{fig_bits}{{3.3}{17}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.3}Map Realizations to GPU Blocks}{17}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.4}Discussion}{17}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces The outline of the simulation application. Disorder realizations are completely independent and can run simultaneously. Each realization contains a unique Monte Carlo parallel tempering process as depicted in Fig.\nobreakspace  {}3.3\hbox {}, and is assigned to a GPU thread block. This task level parallelism yields sufficient number of thread blocks and can fully occupy a parallel computer system. }}{18}}
\newlabel{fig_tasks}{{3.4}{18}}
\newlabel{eq:tsf}{{3.2}{18}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Implementation}{18}}
\newlabel{section_implementation}{{3.4}{18}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}Kernel Organization Optimization}{18}}
\newlabel{section_korg}{{3.4.1}{18}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Three major components of the GPU program. One kernel calls Monte Carlo and parallel tempering, implemented as two device functions. Measurement is implemented as a separate GPU kernel.}}{19}}
\newlabel{fig_korg1}{{3.5}{19}}
\citation{Nguyen:2010:BOS:1884643.1884658,Datta:2008:SCO:1413370.1413375}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces A comparison of the performance of the {\bf  MC-PT integrated} and the {\bf  MC-PT separated} schemes with different numbers of Monte Carlo sweeps between an attempted parallel tempering swap. The test is conducted with a $16^3$ cubic lattice, shared memory probability table of integers, CURAND, and CAMSC. }}{20}}
\newlabel{fig_korg}{{3.6}{20}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.2}Memory Optimization}{20}}
\newlabel{section_memory}{{3.4.2}{20}}
\citation{2008CoPhC.178..208B}
\citation{PhysRevLett.42.1390,Zorn-Hermann-Rebbi-1981}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces The memory access pattern for a single spin update where in addition to the state of the local spin, we also need the states of its 6 neighbors. Periodic boundary conditions are used.}}{21}}
\newlabel{fig_stencil}{{3.7}{21}}
\@writefile{toc}{\contentsline {subsubsection}{Allocation}{21}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.8}{\ignorespaces Unified and separated memory allocation schemes. The unified scheme stores the entire checkerboard lattice together. The separated scheme breaks the memory associated with each sublattice into separate continuous blocks of memory.}}{22}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Unified}}}{22}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Separated}}}{22}}
\newlabel{fig_alloc12}{{3.8}{22}}
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces Performance comparison of the unified/separated/shuffled storage allocation schemes for a $16^3$ lattice. The definition of a transaction is a sequence of 7 loads and a store that serve the spin update.}}{22}}
\newlabel{table_allocation}{{3.1}{22}}
\@writefile{toc}{\contentsline {subsubsection}{Tiling for the Multispin Coding Lattice}{22}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.9}{\ignorespaces The shuffled allocation mixes and integrates two lattices, shown on the top of the figure. The first transformation is taking the A sublattice on the left (blue dots) and the B sublattice on the right (blue circles) to construct an intermediate lattice (middle left figure of blue color). Another intermediate lattice of red color is constructed similarly. We then integrate these two intermediate lattices together, which occupy different bit positions under the compact multispin coding scheme (Section 3.4.3\hbox {}). By using one integer lattice instead of two, we avoid doubling the memory consumption. Also, the memory access pattern is identical to that of the 7-point 3D Jacobi stencil. }}{23}}
\newlabel{fig_alloc3}{{3.9}{23}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.10}{\ignorespaces Memory tiling. The global memory may hold several integer cubes (including one integer per lattice site) if there are more than 32 temperature replicas. The shared memory scratchpad holds the working set of an entire integer cube (no larger than $4 \times 16^3 = 16KB$). The registers hold the data needed for local spin updates.}}{24}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Global Memory}}}{24}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Shared Memory}}}{24}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {Registers}}}{24}}
\newlabel{fig_tile}{{3.10}{24}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.3}Optimizing the Computation}{25}}
\newlabel{section_compute}{{3.4.3}{25}}
\newlabel{eq:e}{{3.3}{25}}
\newlabel{eq:p}{{3.4}{25}}
\newlabel{eq:r}{{3.5}{26}}
\@writefile{toc}{\contentsline {subsubsection}{Probability Look-up Table}{26}}
\newlabel{section_prob}{{3.4.3}{26}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.11}{\ignorespaces The organization of the probability look-up table.}}{26}}
\newlabel{fig_table}{{3.11}{26}}
\citation{curand}
\citation{Salmon:2011:PRN:2063384.2063405}
\citation{2012arXiv1204.6193M}
\@writefile{lof}{\contentsline {figure}{\numberline {3.12}{\ignorespaces A comparison of the overall time consumed per spin flip using four different methods to compute the exponential probability in Eq.\nobreakspace  {}3.4\hbox {} as described in the main text. The experiment is done for a $16^3$ lattice, fp32 CURAND and AMSC1. No parallel-tempering is performed.}}{27}}
\newlabel{fig_perf_prob}{{3.12}{27}}
\@writefile{toc}{\contentsline {subsubsection}{Random Number Generator}{27}}
\newlabel{section_rng}{{3.4.3}{27}}
\@writefile{toc}{\contentsline {subsubsection}{Multispin Coding}{27}}
\newlabel{section_msc}{{3.4.3}{27}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.13}{\ignorespaces A comparison of the overall time required per spin flip using different random number generators. The experiment used a $16^3$ lattice, a shared memory probability table and CAMSC. No parallel-tempering is performed. The loop that consumes random numbers has been unrolled four times to match the four return values of rand123 philox4x32\_7.}}{28}}
\newlabel{fig_rng}{{3.13}{28}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.14}{\ignorespaces This figure demonstrates the computation of $(E \times 2 + S)$ for the purpose of accessing the probability look up table with the deployment of two variations of Asynchronous Multispin Coding (AMSC). Each line in the figure represents an integer, each box of a line represents a bit, and boxes of the same color represent a segment that hold a variable from one of the temperature replicas. We give the name AMSC1 and AMSC3 for these two AMSC schemes according to their segment width. Unlike the AMSC1, the AMSC3 scheme reserves three bits for each segment, and is a less dense storage format. For the calculation of the local energy, we need two spins ($S$) and the coupling ($J$) between them. The $J$ bits and $S$ bits are integrated in the same integer, so that we can fetch both the coupling and the spins using only one memory transaction. The local energy ($e$) of each bond can be calculated by performing two XOR operations. The total local change of energy ($EEE$) is the sum of the contributions from all six nearest neighbors. Since $EEE$ requires three bits for storage, the AMSC1 scheme compute each segment sequentially to avoid overflow, while the AMSC3 scheme can compute multiple segments in parallel. After we obtain $EEE$ in three-bit format, we combine it with the spin state ($S$) by doing string concatenation.}}{29}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {AMSC1}}}{29}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {AMSC3}}}{29}}
\newlabel{fig_msc1}{{3.14}{29}}
\newlabel{fig_camsc}{{3.15(b)}{30}}
\newlabel{sub@fig_camsc}{{(b)}{30}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.15}{\ignorespaces This figure demonstrates how the AMSC4 and CAMSC schemes help exploit bit-level parallelism in computing $(E \times 2 + S)$. Similar to that of the AMSC1 and AMSC3 (see the text and the caption of Fig.\nobreakspace  {}3.14\hbox {}), the XOR operations and summation over six nearest neighbors produces the total local energy ($EEE$). However, since we reserve four bits for each segment, and is capable of holding one more bit over $EEE$, the string concatenation of $EEE$ with $S$ can now be vectorized. The difference between CAMSC and AMSC4 is that $S$ and $J$ are stored in a more compact format. With such a design, CAMSC avoids waste of space and provides much better parallelism in computing $e$. }}{30}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {AMSC4}}}{30}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {CAMSC}}}{30}}
\newlabel{fig_msc2}{{3.15}{30}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.16}{\ignorespaces Comparing the performance using different multispin coding schemes. The experiment is done for a $16^3$ lattice, a shared memory probability table with integers and CURAND. A parallel-tempering move is performed every 10 Metropolis single spin sweeps.}}{31}}
\newlabel{fig_msc_perf}{{3.16}{31}}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Experimental Results}{31}}
\newlabel{section_exp}{{3.5}{31}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.1}The Platform Settings}{31}}
\newlabel{section_platform}{{3.5.1}{31}}
\citation{CSTN-093,2010CoPhC.181.1549B,doi:10.1142/S0129183112400025,Weigel:2012:PPS:2151219.2151631}
\citation{2012arXiv1204.4134J}
\citation{CSTN-093}
\citation{2010CoPhC.181.1549B}
\citation{CSTN-093}
\citation{2010CoPhC.181.1549B}
\citation{PhysRevB.44.5081}
\citation{Katzgraber-Korner-Young-2006}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.2}Performance Evaluation}{32}}
\newlabel{section_compare}{{3.5.2}{32}}
\citation{PhysRevB.44.5081}
\citation{PhysRevB.44.5081}
\citation{Katzgraber-Korner-Young-2006}
\citation{Katzgraber-Korner-Young-2006}
\citation{Janus2-2013}
\@writefile{lof}{\contentsline {figure}{\numberline {3.17}{\ignorespaces Performance comparison with other heterogeneous Ising model simulation programs. \citet  {CSTN-093} reports 4360.1 million Monte Carlo hits per second, which equals to 229 ps/spin. \citet  {2010CoPhC.181.1549B} reports 7977.4 spin flips per microsecond, which equals to 125 ps/spin. }}{33}}
\newlabel{fig_perf}{{3.17}{33}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.3}Simulation Results}{33}}
\newlabel{section_result}{{3.5.3}{33}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.18}{\ignorespaces Comparing the total energy of the $16^3$ sites Ising model with nearest neighbors coupling $J=-1$, to CPU generated results. At each value of the external field, the GPU results are nearly identical to the CPU results. }}{34}}
\newlabel{fig:energy-ising}{{3.18}{34}}
\@writefile{toc}{\contentsline {section}{\numberline {3.6}Conclusion and Future Works}{34}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.19}{\ignorespaces Correlation length vs.\ inverse temperature for the Ising model. The lines from different system sizes cross close to $1/T=0.2217$, which is in agreement with the published result for the critical temperature.\nobreakspace  {}\cite  {PhysRevB.44.5081}}}{35}}
\newlabel{fig:corr-ising}{{3.19}{35}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.20}{\ignorespaces Binder Ratio for the 3D Edwards-Anderson model. The data generated by our GPU code is compared with the data extracted from the paper by Katzgraber {\it  et al.} \cite  {Katzgraber-Korner-Young-2006}}}{35}}
\newlabel{fig:binder-ea}{{3.20}{35}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.21}{\ignorespaces The convergence of the Binder ratio vs.\ number of Monte Carlo steps for the Edwards-Anderson model in a system with $8^3$ sites, with and without parallel tempering for $1/T=2.0$. Parallel tempering dramatically improves the convergence to equilibrium.}}{36}}
\newlabel{fig:qvst}{{3.21}{36}}
\@setckpt{chapters/chap2}{
\setcounter{page}{37}
\setcounter{equation}{5}
\setcounter{enumi}{4}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{3}
\setcounter{section}{6}
\setcounter{subsection}{0}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{21}
\setcounter{table}{1}
\setcounter{NAT@ctr}{0}
\setcounter{subfigure}{0}
\setcounter{lofdepth}{1}
\setcounter{subtable}{0}
\setcounter{lotdepth}{1}
\setcounter{parentequation}{0}
}
