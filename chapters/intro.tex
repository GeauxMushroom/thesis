\chapter{General Introduction}
\section{Computer Simulations}
Computer simulation is the discipline of designing a model of an actual or 
theoretical system, executing the model on a digital computer, and 
analyzing the execution output. Computer simulations have become a useful part of mathematical modeling of 
many natural systems in various disciplines of science. 

In science, typically two types of computer simulations are used. First is a 
numerical simulation of differential equations that cannot be solved 
analytically. In the recent discovery of gravitational waves from a binary
black hole merger, LIGO scientists \cite{PhysRevLett.116.061102} used numerical simulations to provide 
estimates of the mass and spin of the final black hole, the total energy 
radiated in gravitational waves, and the peak gravitational-wave luminosity.
They were also able to verify that their observation is consistent with general 
relativity equations.

Another type is the stochastic simulation. A stochastic simulation is a 
simulation that traces the evolution of variables that can change stochastically
 (randomly) with certain probabilities. With a stochastic model, we create a 
projection which is based on a set of  random values. 
Outputs are recorded and the projection is repeated with a new 
set of random values of the variables. These steps are repeated until a 
sufficient amount of data is gathered. In the end, the distribution of the 
outputs shows the most probable estimates as well as a frame of expectations 
regarding what ranges of values the variables are more or less likely to fall 
in. 

One of the most widely used stochastic simulation methods is the Monte Carlo
methods. Named after the famous casino in Monaco, Monte Carlo methods use
repeated random sampling to obtain numerical results. By the law of large 
numbers, the average of the results obtained from a large number of trials
should be close to the expected value, and gets closer as more test trials
are performed. Therefore, an integral can be evaluated by randomly sampling
the phase space and sum over the samples. 
In physics, Monte Carlo methods are very useful for complex systems such as 
disordered systems, strongly coupled systems, etc. 

\section{Disordered Systems}
In physics, the terms order and disorder designate the presence or absence of 
translational symmetry of a system. The disorder can be put into two main categories 
according to their dynamics: annealed disorder and quenched disorder.

A system is said to present quenched disorder when 
some parameters defining its behavior are random variables which do not evolve 
with time. In these systems, the disorder is explicitly present in the Hamiltonian, 
typically under the form of random coupling $J$ among the degrees of freedom $\sigma$,
\begin{equation}
  \label{eq:4}
  H=H(\sigma,J)
\end{equation}

Spin glasses \cite{Binder-Young-1986} are a classical example of quenched disorder. 
The term spin glass was given to materials that display the lack of long-range
magnetic ordering down to zero temperature in the 1970s. 
Several families of spin glass materials have been identified. The classic examples include 
dilute metallic alloys such as CuMn with 0.9\% Mn, and concentrated insulators 
such as Eu$_x$Sr$_{1-x}$S. In these systems, the quenched disorder originates in the random dilution.
The magnetic moments in Mn and Eu can be described in term of the Heisenberg spins.
An example for real material featuring Ising-like spin is the 
LiHo$_x$Y$_{1-x}$F$_4$ insulator. Since the single-ion crystal field anisotropy
is strong compared to the magnetic interaction between Ho$^{3+}$ ions, the magnetic 
moments can be mapped on to Ising spins that only point parallel or antiparallel
to the c-axis of the tetragonal crystalline structure of the lattice. 

The discovery of the spin glass materials is due to its unique behaviors of
the susceptibility. The now defining signature of spin glass materials is 
the cusp in the low-field AC susceptibility, first found by Cannella and Mydosh(1972).
In contrast to the usual ferromagnetic systems, this signature strongly 
suggests the lack of long range order. The physical picture of the lack 
of divergence in the susceptibility indicates a transition to a state of randomly 
frozen spins. 

In addition to the cusp, some interesting slow dynamic 
behaviors are also observed. For example, the remanent magnetization is found 
if one cools the spin glass in a field to below the transition temperature and 
turns off the field. The magnetization then decays very slowly as it approaches 
zero, signaling a very long relaxation time. Indeed, experimental spin glass
systems can never truly attain equilibrium. 

The simplest model that captures the quenched random magnetic interaction
is the Edwards-Anderson model\cite{Edwards-Anderson1975},
\begin{equation}
  \label{eq:11}
  H=-\sum_{<i,j>}J_{ij}\sigma_i\sigma_j
\end{equation}
mwhere the spins $\sigma_i=\pm 1$ are the degree of freedom, and the random coupling
$J_{ij}$ can be either Gaussian random variables, or binary random variables.

The mean-field variant of this model, the Sherrington-Kirkpatrick model\cite{Sherrington-Kirkpatrick1978,Sherrington-Kirkpatrick-1975}, was 
solved by Parisi\cite{Parisi-1980a,Parisi-1980b,Parisi1980} using the replica 
symmetry breaking approach. In this picture,
there is a hierarchy of the replica overlap, which can be described by an ultrametric
tree. Although this theory has been accepted as the exact solution to an Edwards-Anderson
like model with infinite range interactions, its applicability in lower dimensions 
has been debated. Below the upper critical dimension ($d<6$)\cite{Harris-Lubensky-Chen-1976,Tasaki-1989,Green-Moore-Bray-1983}, 
especially at the most
physically relevant three dimensions, the nature of the spin glass phase is still not clear. 
The main competing picture is the Droplet picture, proposed by Fisher and Huse, in which 
there is only a pair of spin-flip related pure states in the thermal dynamic
limit.


\iffalse
A system is said to present annealed disorder when some parameters entering 
its definition are random variables, but whose evolution is related to that 
of the degrees of freedom defining the system. For example, in the case of 
structural glasses, whose Hamiltonian takes the form,
\begin{equation}
  \label{eq:12}
  H=\sum_{ij}V(r_j-r_j)
\end{equation}
where the degrees of freedom $r_i$ are the positions of the particles, and the 
function $V(r)$ is a potential. Even though there is no quenched disorder in
the Hamiltonian, at low temperature, the system stays in a frozen glassy state, 
and each particle is in a different, disordered environment.
\fi

One of the most significant differences between the two pictures is the effect 
of an external field\cite{Young-Katzgraber2004}. The droplet picture predicts that there is no phase transition
in a field, while the replica symmetry breaking picture predicts that there is a 
transition, and there is an AT line that separates the two phases. 

There have been a lot of intensive numerical studies invested in this problem over 
the last four decades. As we will explain in more detail in this thesis, numerical
simulations of spin glass system present a tremendous challenge. Obtaining the ground
state by minimization method such as branch-and-cut method is mostly useful for 
two dimensions. 

The difficulty of using minimization at three dimensions can be 
considered as consequence of the proof that the ground state energy of a three-dimensional
Edwards-Anderson model is NP-complete. Therefore, the study of the three-dimensional
cases is only practically feasible at finite temperatures. The best available method
is the Monte Carlo. By the very nature of spin glass systems, the long relaxation time, 
Monte Carlo simulation is bound to be very slow. This problem can be attacked from 
two directions, the advancements in algorithms and computer implementations. 
Over the last few decades, different methods to accelerate the Monte Carlo 
have been proposed and tested, we will explain more detail on these methods in this thesis. 
In addition, computer implementations have been improved to shorten the simulation
time. This is not solely on the software programming. Due to the relatively 
simple Monte Carlo method, various dedicated machines were built exclusively for the simulation
of spin glass systems. 

In the following, we will review a few important milestones in the numerical study of
Edwards-Anderson model. Although the model was proposed at 1975, reliable simulations
only appear from around mid-80s.

\citet{Bhatt-Young-1985} studies used Monte Carlo simulations to study the three-dimensional
Edwards-Anderson model in zero field for samples with $3<L<20$. Results for 
$T\ge1.2$ are consistent with a conventional phase transition at $T_c=1.2$.
However, at lower temperatures, the results indicate marginal behavior. This existence
of a spin glass transition has long been considered as an open question due to the
results from this paper. 

\citet{PhysRevB.62.14237} used the parallel tempering technique to study the three-dimensional
Edwards-Anderson model in helicoidal geometry. By measuring the correlation length
in the critical region,  evidence for a second order finite-temperature phase 
transition was obtained and critical exponents such as $\nu$ and $\eta$ were 
calculated. This is the beginning of a new chapter in the numerical simulation 
of spin glass. The wisdom from this paper is that the conventional indicators,
in particular, the ratio of cumulants of the order parameter, are not sufficient 
for detecting a transition. The results from this paper bring to a universal
consensus that the spin glass transition does exist in the three-dimensional
Edwards-Anderson model.

Katzgraber and Young studied the model in a magnetic field--known and 
found the absence of the de Almeida-Thouless line. Later, a one-dimensional 
power-law diluted Ising spin-glass model has been proposed to produce an
effective model at higher dimensions\cite{Young-Katzgraber2004}. Their results for the model 
corresponding to a three-dimensional system are consistent with there being no 
de Almeida-Thouless line.

Recently, it has been suggested that the correlation length may not be a 
good indicator for the model in an external magnetic field.
The Janus collaboration \cite{Banos-2012,TheJanusCollaboration:2012:JFS:2322156.2322158} 
used their special purpose computer with FPGA to simulate
four-dimensional spin glass in a field. They studied the ratio of susceptibilities 
at the two smallest momenta, and found a crossing at finite temperature, which 
indicates that the spin glass phase can exist without time-reversal symmetry
below the upper critical dimension.


\section{Strongly Correlated Systems}
Strongly correlated materials are a wide class of compounds containing ions 
with $d-$ or $f-$orbitals, that show unusual electronic and magnetic properties. 
They include insulators, magnets, paramagnets and superconductors. In transition
metals, such as vanadium, iron, and their oxides, for example, electrons experience
strong Coulombic repulsion because of their spatial configuration, and their 
interaction cannot be described by a static mean field generated by other electrons.
\cite{RevModPhys.56.99,RevModPhys.70.1039}
The interplay of the $d$ and $f$ electrons' internal degree of freedom, such as
spin, charge and orbital moment, can exhibit many interesting ordering phenomena
at low temperatures, and makes strongly correlated systems sensitive to small
changes in external parameters such as temperature, pressure, or doping. 

The most important feature that defines these materials is that the behavior of the electrons
cannot be described in terms of non-interacting entities. Methods that work well
in weakly correlated electron materials, such as Fermi liquid theory, or Density
functional theory (DFT) \cite{RevModPhys.61.689}, are not accurate enough when applied to strongly 
correlated materials. 

Traditionally strongly correlated materials have been described using the model Hamiltonian 
approach, in which the full many-body Hamiltonian is reduced to a simpler, effective
model that retains the essence of the physical phenomena we want to understand.
One of the simplest models is the Hubbard Hamiltonian, 

\begin{equation}
  \label{eq:13}
  H=\sum_{i,j,\sigma}t_{ij}c_{i\sigma}^\dagger c_{j\sigma} + U\sum_i n_{i\uparrow} n_{i\downarrow}
\end{equation}

This Hamiltonian describes electrons with spin directions $\sigma$ moving between 
lattices $i$ and $j$, and they only interact when they meet on the same lattice 
site $i$. The kinetic term favors the delocalization of electrons, while the 
potential term favors localization, and, therefore, they compete against each
other. The system property is then determined by parameters such as the ratio of
the Coulomb interaction $U$ and the bandwidth $W$, the temperature $T$ and the 
hopping or number of electrons. 

One of the most popular approaches used to study the Hubbard model and other related
models is the Dynamical mean field theory (DMFT)
\cite{PhysRevB.45.6479,PhysRevLett.69.168,RevModPhys.68.13,PhysRevLett.69.1236,PhysRevLett.69.1240}. 
In DMFT, a many body lattice
problem is mapped onto a single site impurity problem with effective parameters.
DMFT has been deployed to understand the Mott transition 
\cite{PhysRevLett.69.1796,PhysRevLett.70.1666,PhysRevB.48.7167},
which has been confirmed
by experiments\cite{PhysRevLett.75.105,PhysRevB.58.3690,PhysRevLett.90.186403}. 
DMFT has also been applied to a range of other strongly correlated 
materials. With the computing power growing, and new algorithms and ideas emerging, 
one can expect much a better understanding in complicated strongly correlated 
materials.

\section{Heterogeneous Computing}
%intro
Heterogeneous computing refers to systems that use more than one kind of 
processor or cores. These systems gain performance and efficiency by adding
different processors/accelerators, and divide the task among them to utilize 
their specialized capabilities. 
Popular examples of such accelerators, according to this definition, include 
GPUs, FPGAs, Intel Xeon Phi coprocessors, etc.

%Hardwares
%FPGA?

Graphic processing units, or GPUs, are typically used in computers for image 
processing. 
Due to the performance, it is becoming increasingly common to use a general purpose 
graphics processing unit (GPGPU).
A CPU consists of a few cores optimized for sequential serial processing,
while a GPU has a massively parallel architecture 
consisting of thousands of small cores designed for handling multiple tasks 
simultaneously.
For example, a Nvidia K80 GPU \cite{nv_k80_spec} has 2496 cores, and can achieve up to 2.91 TFlops 
double precision performance and up to 8.74 TFlops single precision performance, 
and has a bandwidth of 480GB/s.
The dominant framework for GPU programming is Nvidia CUDA \cite{Nickolls:2008:SPP:1365490.1365500}, which allows 
programmers to write codes with C, C++ and Fortran and run the program on CUDA-enabled 
GPUs.


Intel Xeon Phi\cite{Jeffers:2013:IXP:2523262} is a coprocessor developed by Intel that features a 
X86-compatible architecture. With up to 61 cores, 244 threads, 
and 1.2 teraFLOPS of performance, a Xeon Phi delivers up to 2.3 times higher 
peak FLOPS than Intel Xeon processor E5 family-based servers.
Since languages, tools, and applications are compatible with both Intel X86 
processor and Intel Xeon Phi coprocessors, it is easier for programmers to
design, write, compile and optimize their code. 


%Pros and Cons when speeding up code within heterogeneous systems.

%Pros:
% performance

Heterogeneous systems are capable of delivering better performance than 
traditional homogeneous systems, by exploits the diversity offered by different 
processors/instruction set architectures(ISAs). 
%http://www.nvidia.com/object/why-choose-tesla.html#sthash.s2ypSuA2.dpuf
The huge performance increase over CPUs makes accelerators popular choices for
supercomputers. Tianhe-2\cite{Liao2014}, a supercomputer developed by China’s National 
University of Defense Technology, leads the list of top 500 supercomputers\cite{top500-2015-11}.
It utilizes 48,000  Intel Xeon Phi coprocessors and 32,000 Ivy Bridge-EP Xeon 
processors to achieve 33.86 petaFLOPS.

% energy efficiency
Heterogeneous systems also have much better energy efficiency. 
The power usage, as well as the cooling cost required to support the computers,
are now the primary cost of ownership for HPC data centers. Therefore,
the HPC community now understands that supercomputers should not be evaluated
solely on the basis of speed, but should also consider metrics related to 
energy efficiency. The most popular metric is the FLOPS/watt.
%[http://www.green500.org/docs/pubs/hp-pac2006.pdf]
By using energy-efficient accelerators,
heterogeneous systems can significantly reduce the energy footprint. 
In fact, heterogeneous accelerator-based systems have been dominating the 
top places of the Green500 \cite{green500},
%[www.green500.org]
a ranking of the most energy-efficient supercomputers in the world. 
In the November 2015 edition of the list\cite{green500-2015-11}, 
the top 40 supercomputers listed all used accelerators of one form or another.

%Cons:
% Complexity in programming, utilization,
Albeit the advantages in the hardware, the heterogeneous nature present new
and unique challenges for programmers and scientists. 
%Parallelism
In parallel programming, programmers need to explore the problem for the
possibility of parallelization, map the tasks onto threads, to schedule for
communication and/or synchronization, and to solve problems such as race conditions,
etc. Communication and barriers could result in parallel slow-down, 
a situation in which the overhead from communication outweighs the performance 
gain from parallelism, and further parallelization increases the time to finish
the workload. 
Accelerators use a huge number of cores to achieve great performance. 
To utilize all the cores, a lot of parallelisms is required, and efficient 
parallelism is critical. 

%memory architecture
Different processors/accelerators often feature different memory architecture.
To achieve the best possible performance, architecture aware memory access is 
very important to utilize the high memory bandwidth and reduce the latency.
This is even more important in programming for accelerators such as GPUs than in
CPU programming, since the latency is not hidden by a large cache.
For example, in CUDA %, the architecture Nvidia use for programming their GPUs, 
there are different types of memory such as registers, local memory, shared 
memory, global memory, and constant memory, etc. Each of these types is 
different in terms of size, latency, bandwidth, and performance profile for 
various access modes, and one has to make conscious decisions when using them
in order to avoid performance penalties and make the best of the hardware. 
In CUDA, this means programmers need to put the correct qualifiers in front when 
declaring their variables and move their variables around the memory hierarchy 
when needed.
 
%Communication
%Since the accelerators are usually connected to the CPU via the PCI express bus,their communication bandwidth with the CPU are limited.
%CPU-accelerator communication
%Inter-accelerator communication

% Portability, compilers and tools
The different architecture of processors makes it hard to write portable codes.
First, some programming languages, such as CUDA, and associated compilers and tools,
 are exclusive for their specific platforms. In order to program, debug, profile 
and optimize a code for different platforms, programmers often need to understand 
more than one set of tools. 
Second, the performance profiles for different hardware are very
different from each other, thus, an efficient code on one platform may not be
optimal for another. 
Fortunately, tools that aim at portable accelerated codes with great performance, such as 
OpenMP\cite{dagum1998openmp}, OpenCL\cite{Stone:2010:OPP:622179.1803953} and OpenACC\cite{OpenACC}, are evolving along with the hardware, and they allow programmers
to worry more about the problem rather than the language and hardware they are
using, and make it much easier to develop and maintain codes that run on heterogeneous systems.




%Programming models

\section{Scope and Structure}
In this dissertation, I cover my work in two projects.

The first is the work on the Three-Dimensional Edwards-Anderson Model in an External
field. We first discuss the model and the theoretical understanding in chapter 
\ref{chap:SGintro}. Then, an efficient GPU implementation is described in chapter
\ref{chap:SG_imp}. We show the results obtained from this study in chapter \ref{chap:sg_result}.
Some additional research using the covariance matrix is covered in the appendix.

Second, we show our work on a Continuous-Time Quantum Monte Carlo solver implemented
on the Intel Xeon Phi platform in chapter \ref{chap:cthyb}. We first discuss the 
motivation and formalism behind this implementation. Then, we show the detail of
the implementation and the optimization. We also included some preliminary benchmarking
results.



%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../thesis"
%%% End:
