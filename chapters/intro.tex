\chapter{Introduction}
\section{Computer Simulations}
Computer simulation is the discipline of designing a model of an actual or 
theoretical system, executing the model on a digital computer, and 
analyzing the execution output. Computer simulations have become a useful part of mathematical modeling of 
many natural systems in various disciplines of science. 

In science, typically two types of computer simulations are used. First is a 
numerical simulation of differential equations that cannot be solved 
analytically. In the recent discovery of gravitational waves from a binary
black hole merger, LIGO scientists used numerical simulations to provide 
estimates of the mass and spin of the final black hole, the total energy 
radiated in gravitational waves, and the peak gravitational-wave luminosity.
They were also able to verify that their observation is consistent with general 
relativity equations.

Another type is the stochastic simulation. A stochastic simulation is a 
simulation that traces the evolution of variables that can change stochastically
 (randomly) with certain probabilities. With a stochastic model we create a 
projection which is based on a set of  random values. 
Outputs are recorded and the projection is repeated with a new 
set of random values of the variables. These steps are repeated until a 
sufficient amount of data is gathered. In the end, the distribution of the 
outputs shows the most probable estimates as well as a frame of expectations 
regarding what ranges of values the variables are more or less likely to fall 
in. 

One of the most widely used stochastic simulation methods is the Monte Carlo
methods. Named after the famous casino in Monaco, Monte Carlo methods use
repeated random sampling to obtain numerical results. By the law of large 
numbers, the average of the results obtained from a large number of trials
should be close the the expected value, and gets closer as more test trials
are performed. Therefore, an integral can be evaluated by randomly sampling
the phase space and sum over the samples. 
In physics, Monte Carlo methods are very useful for complex systems such as 
disordered systems, strongly coupled systems, etc. 

\section{Disordered Systems}
In physics, the terms order and disorder designate the presence or absence of 
some symmetry or correlation in a many-particle system. 
The disorder can be put into two main categories according to their dynamics: 
annealed disorder and quenched disorder.

In statistical physics, a system is said to present quenched disorder when 
some parameters defining its behavior are random variables which do not evolve 
with time. In these systems, the disorder is explicitly present in the Hamiltonian, 
typically under the form of random coupling $J$ among the degrees of freedom $\sigma$,
\begin{equation}
  \label{eq:4}
  H=H(\sigma,J)
\end{equation}
Spin glasses are a classical example of quenched disorder. The most famous model 
is the Edwards-Anderson model,
\begin{equation}
  \label{eq:11}
  H=-\sum_{<i,j>}J_{ij}\sigma_i\sigma_j
\end{equation}
where the spins $\sigma_i=\pm 1$ are the degree of freedom, and the random coupling
$J_{ij}$ can be either Gaussian random variables, or binary random variables.


A system is said to present annealed disorder when some parameters entering 
its definition are random variables, but whose evolution is related to that 
of the degrees of freedom defining the system. For example, in the case of 
structural glasses, whose Hamiltonian takes the form,
\begin{equation}
  \label{eq:12}
  H=\sum_{ij}V(r_j-r_j)
\end{equation}
where the degrees of freedom $r_i$ are the positions of the particles, and the 
function $V(r)$ is a potential. Even though there is no quenched disorder in
the Hamiltonian, at low temperature, the system stays in a frozen glassy state, 
and each particle is in a different, disordered environment.


%Define what kind of disorder in real systems. They are most from dilution or 
%doping, giving examples. 

%In real materials, disorder is often introduced by 


\section{Strongly Correlated Systems}
Strongly correlated materials are a wide class of compounds containing ions 
with $d-$ or $f-$orbitals, that show unusual electronic and magnetic properties. 
They include insulators, magnets, paramagnets and superconductors. In transition
metals, such as vanadium, iron, and their oxides, for example, electrons experience
strong Coulombic repulsion because of their spatial configuration, and their 
interaction cannot be described by a static mean field generated by other electrons.
The interplay of the $d$ and $f$ electrons' internal degree of freedom, such as
spin, charge and orbital moment, can exhibit many interesting ordering phenomena
at low temperatures, and makes strongly correlated systems sensitive to small
changes to external parameters such as temperature, pressure, or doping. 

The most important feature that defines these materials is that the behavior of the electrons
cannot be described in terms of non-interacting entities. Methods that work well
in weakly correlated electron materials, such as Fermi liquid theory, or Density
functional theory (DFT), are not accurate enough when applied to strongly 
correlated materials. 

Traditionally strong correlated materials have been described using the model Hamiltonian 
approach, in which the full many-body Hamiltonian is reduced to a simpler, effective
model that retians the essence of the physical phenomena we want to understand.
One of the simplest models is the Hubbard Hamiltonian, 

\begin{equation}
  \label{eq:13}
  H=\sum_{i,j,\sigma}t_{ij}c_{i\sigma}^\dagger c_{j\sigma} + U\sum_i n_{i\uparrow} n_{i\downarrow}
\end{equation}

This Hamiltonian describes electrons with spin directions $\sigma$ moving between 
lattices $i$ and $j$, and they only interact when they meet on the same lattice 
site $i$. The kinetic term favors the delocalization of electrons, while the 
potential term favors localization, and therefore they compete against each
other. The system property is then determined by parameters such as the ratio of
the Coulomb interaction $U$ and the bandwidth $W$, the temperature $T$ and the 
hopping or number of electrons. 

One of the most popular approach used to study the Hubbard model and other related
models is the Dynamical mean field theory (DMFT). In DMFT, a many body lattice
problem is mapped onto a single site impurity problem with effective parameters.
DMFT has been deployed to understand the Mott transition, which has been confirmed
by experiments. DMFT has also been applied to a range of strongly correlated 
materials. With the computing power growing, and new algorithms and ideas emerging, 
one can expect much a better understanding in complicated strongly correlated 
materials.

\section{Heterogeneous Computing}
%intro
Heterogeneous computing refers to systems that use more than one kind of 
processor or cores. These system gain performance and efficiency by adding
different processors/accelerators, and divide the task among them to utilize 
their specialized capabilities. 
Popular examples of such accelerators, according to this definition, include 
GPUs, FPGAs, Intel Xeon Phi coprocessors, etc.

%Hardwares
%FPGA?

Graphic processing units, or GPUs, are typically used in computers for image 
processing. 
Due to the performance, it is becoming increasingly common to use a general purpose 
graphics processing unit (GPGPU).
A CPU consists of a few cores optimized for sequential serial processing,
while a GPU has a massively parallel architecture 
consisting of thousands of small cores designed for handling multiple tasks 
simultaneously.
For example, a Nvidia K80 GPU has 2496 cores, and can achieve up to 2.91 TFlops 
double precision performance and up to 8.74 TFlops single precision performance, 
and has a bandwidth of 480GB/s.
The dominant framework in GPU programming is Nvidia CUDA, which allows 
programmers to write codes with C, C++ and Fortran and run the program on CUDA
enabled GPUs.


Intel Xeon Phi is a coprocessor developed by Intel that features a 
X86-compatible architecture. With up to 61 cores, 244 threads, 
and 1.2 teraFLOPS of performance, a Xeon Phi delivers up to 2.3 times higher 
peak FLOPS than Intel Xeon processor E5 family-based servers.
Since languages, tools, and applications are compatible for both Intel Xeon 
processor and Intel Xeon Phi coprocessors, it is easier for programmers to
design, write, compile and optimize their code. 


%Pros and Cons when speeding up code within heterogeneous systems.

%Pros:
% performance

Heterogeneous systems are capable of delivering better performance than 
traditional homogeneous systems, by exploits the diversity offered by different 
processors/instruction set architectures(ISAs). 
%http://www.nvidia.com/object/why-choose-tesla.html#sthash.s2ypSuA2.dpuf
The huge performance increase over CPUs make accelerators popular choices for
supercomputers. Tianhe-2, a supercomputer developed by Chinaâ€™s National 
University of Defense Technology, leads the list of top 500 supercomputers.
It utilizes 48,000  Intel Xeon Phi coprocessors and 32,000 Ivy Bridge-EP Xeon 
processors to achieve 33.86 petaFLOPS.

% energy efficiency
Heterogeneous systems also have much better energy efficiency. 
The power usage as well as the cooling cost required to support the computers
are now the primary cost of ownership for HPC data centers. Therefore,
the HPC community now understands that supercomputers should not be evaluated
solely on the basis of speed, but should also consider metrics related to 
energy efficiency. The most popular metric is the FLOPS/watt.
%[http://www.green500.org/docs/pubs/hp-pac2006.pdf]
By using energy-efficient accelerators,
heterogeneous systems can significantly reduce the energy-footprint. 
In fact, heterogeneous accelerator-based systems have been dominating the 
top places of the Green500, %[www.green500.org]
a ranking of the most energy-efficient supercomputers in the world. 
In the November 2015 edition of the list, 
the top 40 supercomputers listed all used accelerators of one form or another.

%Cons:
% Complexity in programming, utilization,
Albeit the advantages in the hardware, the heterogeneous nature present new
and unique challenges for programmers and scientists. 
%Parallelism
In parallel programming, programmers need to explore the problem for 
possibility of parallelization, map the tasks onto threads, schedule for
communication and/or synchronization, and solve problems such as race conditions,
etc. Communication and barriers could result in parallel slow-down, 
a situation in which the overhead from communication outweighs the performance 
gain from parallelism, and further parallelization increases the time to finish
the workload. 
Accelerators use a huge number of cores to achieve great performance. 
To utilize all the cores, a lot of parallelism is required, and efficient 
parallelism is critical. 

%memory architecture
Different processors/accelerators often feature different memory architecture.
To achieve the best possible performance, architecture aware memory access is 
very important to utilize the high memory bandwidth and reduce the latency.
This is even more important in programming for accelerators such as GPUs than in
CPU programming, since the latency is not hidden by large cache.
For example, in CUDA %, the architecture Nvidia use for programming their GPUs, 
there are different types of memory such as registers, local memory, shared 
memory, global memory, and constant memory, etc. Each of these types are 
different in terms of size, latency, bandwidth, and performance profile for 
various access modes, and one have to make conscious decisions when using them
in order to avoid performance penalties and make the best of the hardware. 
In CUDA, this means programmers need to put the correct qualifiers in front when 
declaring their variables, and move their variables around the memory hierarchy 
when needed.
 
%Communication
%Since the accelerators are usually connected to the CPU via the PCI express bus,their communication bandwidth with the CPU are limited.
%CPU-accelerator communication
%Inter-accelerator communication

% Portability, compilers and tools
The different architecture of processors makes it hard to write portable codes.
First, some programming languages, such as CUDA, and associated compilers and tools,
 are exclusive for their specific platforms. In order to program, debug, profile 
and optimize a code for different platforms, programmers often need to understand 
more than one set of tools. 
Second, different performance profile for different hardwares are very
different from each other, thus an efficient code on one platform may not be
optimal for another. 
Fortunately, tools that aim at portable accelerated codes with great performance, such as 
OpenACC and OpenMP, are evolving along with the hardware, and they allows programmers
to worry more about the problem rather than the language and hardware they are
using, and make it much easier to develop and maintain codes that run on heterogeneous systems.




%Programming models

\section{Scope and Structure}
In this dissertation, I cover my work in two projects.

The first is the work on the Three Dimensional Edwards-Anderson Model in an External
field. We first discuss the model and the theoretical understanding in chapter 
\ref{chap:SGintro}. Then, an efficient GPU implementation is described in chapter
\ref{chap:sg_imp}. We show the results obtained from this study in chapter \ref{chap:sg_result}.
Some additional research using the covariance matrix is covered in the appendix.

Second, we show our work on an Continuous Time Quantum Monte Carlo solver implemented
on the Intel Xeon Phi platform in chapter \ref{chap:cthyb}. We first discuss the 
motivation and formalism behind this implementation. Then, we show the detail of
the implementation and the optimization. We also included some preliminary benchmarking
results.



%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../thesis"
%%% End:
